"""
Module Training Models cho PhÃ¡t hiá»‡n Gian láº­n Tháº» TÃ­n dá»¥ng

Module nÃ y cung cáº¥p cÃ¡c hÃ m Ä‘á»ƒ train (huáº¥n luyá»‡n) cÃ¡c mÃ´ hÃ¬nh Ensemble Learning:
- Random Forest: Rá»«ng ngáº«u nhiÃªn - táº¡o nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh
- AdaBoost: Adaptive Boosting - káº¿t há»£p nhiá»u models yáº¿u
- XGBoost: Extreme Gradient Boosting - gradient boosting tá»‘i Æ°u

Giáº£i thÃ­ch Ä‘Æ¡n giáº£n:
- Train model = dáº¡y mÃ¡y tÃ­nh há»c tá»« dá»¯ liá»‡u Ä‘á»ƒ nháº­n biáº¿t gian láº­n
- Ensemble = káº¿t há»£p nhiá»u models Ä‘á»ƒ cÃ³ káº¿t quáº£ tá»‘t hÆ¡n
- LÆ°u model = lÆ°u láº¡i model Ä‘Ã£ train Ä‘á»ƒ dÃ¹ng sau (khÃ´ng cáº§n train láº¡i)
"""

import pickle
import numpy as np
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb
from sklearn.metrics import classification_report
from sklearn import __version__ as sklearn_version


def train_random_forest(X_train, y_train, n_estimators=200, max_depth=15, 
                        class_weight=None, random_state=42, n_jobs=-1, verbose=1):
    """
    Train (huáº¥n luyá»‡n) mÃ´ hÃ¬nh Random Forest
    
    Random Forest lÃ  gÃ¬?
    - Táº¡o nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh (decision trees)
    - Má»—i cÃ¢y Ä‘Æ°a ra má»™t dá»± Ä‘oÃ¡n
    - Láº¥y káº¿t quáº£ trung bÃ¬nh hoáº·c Ä‘a sá»‘ tá»« táº¥t cáº£ cÃ¢y
    - Giá»‘ng nhÆ° há»i nhiá»u chuyÃªn gia vÃ  láº¥y Ã½ kiáº¿n Ä‘a sá»‘
    
    Táº¡i sao dÃ¹ng Random Forest?
    - Máº¡nh máº½, Ã­t bá»‹ overfitting
    - Xá»­ lÃ½ tá»‘t dá»¯ liá»‡u máº¥t cÃ¢n báº±ng (vá»›i class_weight)
    - Dá»… hiá»ƒu vÃ  giáº£i thÃ­ch
    
    Parameters:
    -----------
    X_train : array-like
        Dá»¯ liá»‡u Ä‘á»ƒ train (features - cÃ¡c Ä‘áº·c trÆ°ng)
    y_train : array-like
        NhÃ£n (labels - 0 = bÃ¬nh thÆ°á»ng, 1 = gian láº­n)
    n_estimators : int, máº·c Ä‘á»‹nh=200
        Sá»‘ cÃ¢y trong rá»«ng (nhiá»u hÆ¡n = tá»‘t hÆ¡n nhÆ°ng cháº­m hÆ¡n)
    max_depth : int, máº·c Ä‘á»‹nh=15
        Äá»™ sÃ¢u tá»‘i Ä‘a cá»§a má»—i cÃ¢y (sÃ¢u hÆ¡n = phá»©c táº¡p hÆ¡n)
    class_weight : dict hoáº·c 'balanced', tÃ¹y chá»n
        Trá»ng sá»‘ cho cÃ¡c lá»›p (Ä‘á»ƒ xá»­ lÃ½ máº¥t cÃ¢n báº±ng dá»¯ liá»‡u)
    random_state : int, máº·c Ä‘á»‹nh=42
        Sá»‘ ngáº«u nhiÃªn Ä‘á»ƒ Ä‘áº£m báº£o káº¿t quáº£ giá»‘ng nhau má»—i láº§n cháº¡y
    n_jobs : int, máº·c Ä‘á»‹nh=-1
        Sá»‘ CPU cores Ä‘á»ƒ sá»­ dá»¥ng (-1 = dÃ¹ng táº¥t cáº£)
    verbose : int, máº·c Ä‘á»‹nh=1
        Má»©c Ä‘á»™ hiá»ƒn thá»‹ thÃ´ng tin (0 = im láº·ng, 1 = hiá»ƒn thá»‹)
    
    Returns:
    --------
    model : RandomForestClassifier
        Model Random Forest Ä‘Ã£ Ä‘Æ°á»£c train (sáºµn sÃ ng Ä‘á»ƒ dá»± Ä‘oÃ¡n)
    """
    if verbose >= 1:
        print("ðŸ”¹ Training Random Forest model...")
        print(f"   Parameters: n_estimators={n_estimators}, max_depth={max_depth}")
    
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        class_weight=class_weight,
        random_state=random_state,
        n_jobs=n_jobs
    )
    
    model.fit(X_train, y_train)
    
    if verbose >= 1:
        print("âœ… Random Forest training completed!")
    
    return model


def train_adaboost(X_train, y_train, n_estimators=50, learning_rate=1.0,
                   base_estimator=None, random_state=42, verbose=1):
    """
    Train (huáº¥n luyá»‡n) mÃ´ hÃ¬nh AdaBoost
    
    AdaBoost lÃ  gÃ¬?
    - Adaptive Boosting - TÄƒng cÆ°á»ng thÃ­ch á»©ng
    - Train nhiá»u models yáº¿u (weak learners)
    - Models sau há»c tá»« lá»—i cá»§a models trÆ°á»›c
    - Káº¿t há»£p táº¥t cáº£ models Ä‘á»ƒ cÃ³ káº¿t quáº£ tá»‘t
    - Giá»‘ng nhÆ° há»c tá»« sai láº§m vÃ  cáº£i thiá»‡n dáº§n
    
    Táº¡i sao dÃ¹ng AdaBoost?
    - Hiá»‡u quáº£ vá»›i dá»¯ liá»‡u phá»©c táº¡p
    - Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh trá»ng sá»‘ cho cÃ¡c máº«u khÃ³
    - ThÆ°á»ng cho káº¿t quáº£ tá»‘t
    
    Parameters:
    -----------
    X_train : array-like
        Dá»¯ liá»‡u Ä‘á»ƒ train (features)
    y_train : array-like
        NhÃ£n (0 = bÃ¬nh thÆ°á»ng, 1 = gian láº­n)
    n_estimators : int, máº·c Ä‘á»‹nh=50
        Sá»‘ models yáº¿u (nhiá»u hÆ¡n = tá»‘t hÆ¡n nhÆ°ng cháº­m hÆ¡n)
    learning_rate : float, máº·c Ä‘á»‹nh=1.0
        Tá»‘c Ä‘á»™ há»c (nhá» hÆ¡n = há»c cháº­m hÆ¡n nhÆ°ng á»•n Ä‘á»‹nh hÆ¡n)
    base_estimator : object, tÃ¹y chá»n
        Model cÆ¡ sá»Ÿ (máº·c Ä‘á»‹nh: Decision Tree Ä‘Æ¡n giáº£n)
    random_state : int, máº·c Ä‘á»‹nh=42
        Sá»‘ ngáº«u nhiÃªn Ä‘á»ƒ Ä‘áº£m báº£o káº¿t quáº£ giá»‘ng nhau
    verbose : int, máº·c Ä‘á»‹nh=1
        Má»©c Ä‘á»™ hiá»ƒn thá»‹ thÃ´ng tin
    
    Returns:
    --------
    model : AdaBoostClassifier
        Model AdaBoost Ä‘Ã£ Ä‘Æ°á»£c train
    """

    if verbose >= 1:
        print("ðŸ”¹ Training AdaBoost model...")
        print(f"   Parameters: n_estimators={n_estimators}, learning_rate={learning_rate}")

    # Default weak learner
    if base_estimator is None:
        base_estimator = DecisionTreeClassifier(max_depth=1, random_state=random_state)

    # ðŸ”¥ Sá»­a lá»—i: chá»‰ dÃ¹ng 'estimator=' (báº£n Ä‘Ãºng cá»§a sklearn má»›i)
    model = AdaBoostClassifier(
        estimator=base_estimator,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        random_state=random_state
    )

    model.fit(X_train, y_train)

    if verbose >= 1:
        print("âœ… AdaBoost training completed!")
    
    return model



def train_xgboost(X_train, y_train, n_estimators=100, max_depth=6, 
                  learning_rate=0.1, scale_pos_weight=None, random_state=42, 
                  verbose=1, use_label_encoder=False):
    """
    Train (huáº¥n luyá»‡n) mÃ´ hÃ¬nh XGBoost
    
    XGBoost lÃ  gÃ¬?
    - Extreme Gradient Boosting
    - Gradient Boosting Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a ráº¥t máº¡nh
    - Train nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh theo thá»© tá»±
    - CÃ¢y sau sá»­a lá»—i cá»§a cÃ¢y trÆ°á»›c
    - Ráº¥t nhanh vÃ  máº¡nh, thÆ°á»ng cho káº¿t quáº£ tá»‘t nháº¥t
    
    Táº¡i sao dÃ¹ng XGBoost?
    - ThÆ°á»ng cho káº¿t quáº£ tá»‘t nháº¥t trong cÃ¡c competitions
    - Xá»­ lÃ½ tá»‘t dá»¯ liá»‡u máº¥t cÃ¢n báº±ng (vá»›i scale_pos_weight)
    - Nhanh vÃ  hiá»‡u quáº£
    
    Parameters:
    -----------
    X_train : array-like
        Dá»¯ liá»‡u Ä‘á»ƒ train (features)
    y_train : array-like
        NhÃ£n (0 = bÃ¬nh thÆ°á»ng, 1 = gian láº­n)
    n_estimators : int, máº·c Ä‘á»‹nh=100
        Sá»‘ cÃ¢y (boosting rounds) - nhiá»u hÆ¡n = tá»‘t hÆ¡n
    max_depth : int, máº·c Ä‘á»‹nh=6
        Äá»™ sÃ¢u tá»‘i Ä‘a cá»§a má»—i cÃ¢y
    learning_rate : float, máº·c Ä‘á»‹nh=0.1
        Tá»‘c Ä‘á»™ há»c (nhá» hÆ¡n = há»c cháº­m hÆ¡n nhÆ°ng á»•n Ä‘á»‹nh hÆ¡n)
    scale_pos_weight : float, tÃ¹y chá»n
        Trá»ng sá»‘ cho lá»›p gian láº­n (Ä‘á»ƒ xá»­ lÃ½ máº¥t cÃ¢n báº±ng)
        VÃ­ dá»¥: náº¿u cÃ³ 100 bÃ¬nh thÆ°á»ng vÃ  1 gian láº­n, scale_pos_weight = 100
    random_state : int, máº·c Ä‘á»‹nh=42
        Sá»‘ ngáº«u nhiÃªn Ä‘á»ƒ Ä‘áº£m báº£o káº¿t quáº£ giá»‘ng nhau
    verbose : int, máº·c Ä‘á»‹nh=1
        Má»©c Ä‘á»™ hiá»ƒn thá»‹ thÃ´ng tin
    use_label_encoder : bool, máº·c Ä‘á»‹nh=False
        CÃ³ dÃ¹ng label encoder khÃ´ng (khÃ´ng dÃ¹ng trong phiÃªn báº£n má»›i)
    
    Returns:
    --------
    model : XGBClassifier
        Model XGBoost Ä‘Ã£ Ä‘Æ°á»£c train
    """
    if verbose >= 1:
        print("ðŸ”¹ Training XGBoost model...")
        print(f"   Parameters: n_estimators={n_estimators}, max_depth={max_depth}, learning_rate={learning_rate}")
    
    model = xgb.XGBClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        learning_rate=learning_rate,
        scale_pos_weight=scale_pos_weight,
        random_state=random_state,
        use_label_encoder=use_label_encoder,
        eval_metric='logloss'
    )
    
    model.fit(X_train, y_train)
    
    if verbose >= 1:
        print("âœ… XGBoost training completed!")
    
    return model


def save_model(model, model_name, save_dir='models'):
    """
    Save trained model to pickle file.
    
    Parameters:
    -----------
    model : object
        Trained model to save
    model_name : str
        Name of the model (will be used as filename)
    save_dir : str or Path, default='models'
        Directory to save the model
    
    Returns:
    --------
    save_path : Path
        Path where model was saved
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # Clean model name for filename
    filename = model_name.lower().replace(' ', '_') + '.pkl'
    save_path = save_dir / filename
    
    with open(save_path, 'wb') as f:
        pickle.dump(model, f)
    
    print(f"âœ… Model saved to {save_path}")
    return save_path


def load_model(model_name, model_dir='models'):
    """
    Load trained model from pickle file.
    
    Parameters:
    -----------
    model_name : str
        Name of the model (filename without .pkl)
    model_dir : str or Path, default='models'
        Directory containing the model
    
    Returns:
    --------
    model : object
        Loaded model
    """
    model_dir = Path(model_dir)
    filename = model_name.lower().replace(' ', '_') + '.pkl'
    model_path = model_dir / filename
    
    if not model_path.exists():
        raise FileNotFoundError(f"Model not found: {model_path}")
    
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    print(f"âœ… Model loaded from {model_path}")
    return model


def evaluate_model_performance(model, X_test, y_test, model_name='Model'):
    """
    Evaluate model and print classification report.
    
    Parameters:
    -----------
    model : object
        Trained model with predict() method
    X_test : array-like
        Test features
    y_test : array-like
        Test labels
    model_name : str, default='Model'
        Name of the model for display
    """
    print(f"\n{'='*60}")
    print(f"Evaluation Results for {model_name}")
    print(f"{'='*60}")
    
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))
    
    return y_pred
