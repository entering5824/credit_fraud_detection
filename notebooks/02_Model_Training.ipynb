{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4400ff",
   "metadata": {},
   "source": [
    "# Model Training v√† Testing Pipeline\n",
    "\n",
    "Notebook n√†y th·ª±c hi·ªán:\n",
    "1. Load preprocessed data t·ª´ notebook 01\n",
    "2. Test pipeline v·ªõi m·ªôt v√†i m√¥ h√¨nh nh·ªè ƒë·ªÉ verify\n",
    "3. Train c√°c ensemble models (s·∫Ω ƒë∆∞·ª£c ho√†n thi·ªán sau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08493449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get project root directory\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir\n",
    "if (current_dir / 'src').exists():\n",
    "    project_root = current_dir\n",
    "elif (current_dir.parent / 'src').exists():\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "# Add project root to path\n",
    "project_root_str = str(project_root.absolute())\n",
    "if project_root_str not in sys.path:\n",
    "    sys.path.insert(0, project_root_str)\n",
    "\n",
    "# Import modules\n",
    "from src.data_preprocessing import scale_features, split_data, apply_smote, get_class_weights\n",
    "from src.evaluate import evaluate_model, get_metrics_dict\n",
    "\n",
    "# Import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(f\"Project root: {project_root.absolute()}\")\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efb616",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "Load d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c preprocess t·ª´ notebook 01. \n",
    "**L∆∞u √Ω**: C·∫ßn ch·∫°y notebook 01 tr∆∞·ªõc ƒë·ªÉ c√≥ d·ªØ li·ªáu ƒë√£ preprocess.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "# Note: In a real scenario, you would load the preprocessed data from notebook 01\n",
    "# For testing, we'll reload and preprocess the data here\n",
    "\n",
    "data_path = project_root / 'data' / 'creditcard.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [f'V{i}' for i in range(1, 29)] + ['Amount']\n",
    "X = df[feature_cols]\n",
    "y = df['Class']\n",
    "\n",
    "print(f\"Data loaded: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "print(f\"Class distribution: {y.value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X_scaled, scaler = scale_features(X, feature_cols=feature_cols, fit=True)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    X_scaled, y, \n",
    "    test_size=0.15, \n",
    "    val_size=0.15, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n",
    "print(f\"Train: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Validation: {X_val.shape[0]:,} samples\")\n",
    "print(f\"Test: {X_test.shape[0]:,} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb2abb0",
   "metadata": {},
   "source": [
    "## 2. Test Pipeline v·ªõi M√¥ h√¨nh Nh·ªè\n",
    "\n",
    "Test pipeline v·ªõi Logistic Regression v√† Random Forest nh·ªè ƒë·ªÉ verify m·ªçi th·ª© ho·∫°t ƒë·ªông ƒë√∫ng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ad75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Logistic Regression v·ªõi class weights\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Logistic Regression with class weights\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get class weights\n",
    "class_weights = get_class_weights(y_train)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Train model\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight=class_weights,\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_val)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "metrics_lr, figures_lr = evaluate_model(\n",
    "    y_val, y_pred_lr, y_pred_proba_lr,\n",
    "    model_name='Logistic Regression',\n",
    "    plot_cm=True,\n",
    "    plot_roc=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Random Forest nh·ªè\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Random Forest (small)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=50,  # Small for testing\n",
    "    max_depth=10,\n",
    "    class_weight=class_weights,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_val)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "metrics_rf, figures_rf = evaluate_model(\n",
    "    y_val, y_pred_rf, y_pred_proba_rf,\n",
    "    model_name='Random Forest',\n",
    "    plot_cm=True,\n",
    "    plot_roc=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925f21e",
   "metadata": {},
   "source": [
    "## 3. Verify Pipeline Ho·∫°t ƒë·ªông\n",
    "\n",
    "Ki·ªÉm tra xem pipeline c√≥ ho·∫°t ƒë·ªông ƒë√∫ng kh√¥ng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00909890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for comparison\n",
    "test_metrics = [\n",
    "    get_metrics_dict(y_val, y_pred_lr, y_pred_proba_lr, 'Logistic Regression'),\n",
    "    get_metrics_dict(y_val, y_pred_rf, y_pred_proba_rf, 'Random Forest')\n",
    "]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(test_metrics)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Pipeline Test Results - Metrics Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline test completed successfully!\")\n",
    "print(\"‚úÖ All models trained and evaluated correctly\")\n",
    "print(\"‚úÖ Evaluation functions working properly\")\n",
    "print(\"\\nNext steps: Train full ensemble models (Random Forest, AdaBoost, XGBoost)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0495b",
   "metadata": {},
   "source": [
    "## 4. Train c√°c Ensemble Models ƒë·∫ßy ƒë·ªß\n",
    "\n",
    "Train 3 ensemble models: Random Forest, AdaBoost, XGBoost v·ªõi parameters t·ªëi ∆∞u.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model training functions\n",
    "from src.models import (\n",
    "    train_random_forest, train_adaboost, train_xgboost,\n",
    "    save_model, evaluate_model_performance\n",
    ")\n",
    "\n",
    "# Dictionary to store all models and results\n",
    "trained_models = {}\n",
    "model_predictions = {}\n",
    "model_probabilities = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ad90b",
   "metadata": {},
   "source": [
    "### 4.1 Train Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793fd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest v·ªõi parameters t·ªëi ∆∞u\n",
    "rf_model = train_random_forest(\n",
    "    X_train, y_train,\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    class_weight=class_weights,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Predictions on validation set\n",
    "y_pred_rf = rf_model.predict(X_val)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Store results\n",
    "trained_models['Random Forest'] = rf_model\n",
    "model_predictions['Random Forest'] = y_pred_rf\n",
    "model_probabilities['Random Forest'] = y_pred_proba_rf\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model_performance(rf_model, X_val, y_val, 'Random Forest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451e238",
   "metadata": {},
   "source": [
    "### 4.2 Train AdaBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AdaBoost\n",
    "ada_model = train_adaboost(\n",
    "    X_train, y_train,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Predictions on validation set\n",
    "y_pred_ada = ada_model.predict(X_val)\n",
    "y_pred_proba_ada = ada_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Store results\n",
    "trained_models['AdaBoost'] = ada_model\n",
    "model_predictions['AdaBoost'] = y_pred_ada\n",
    "model_probabilities['AdaBoost'] = y_pred_proba_ada\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model_performance(ada_model, X_val, y_val, 'AdaBoost')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b586f7d",
   "metadata": {},
   "source": [
    "### 4.3 Train XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight for XGBoost (ratio of negative to positive class)\n",
    "n_negative = (y_train == 0).sum()\n",
    "n_positive = (y_train == 1).sum()\n",
    "scale_pos_weight = n_negative / n_positive if n_positive > 0 else 1.0\n",
    "\n",
    "print(f\"Scale pos weight for XGBoost: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model = train_xgboost(\n",
    "    X_train, y_train,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Predictions on validation set\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Store results\n",
    "trained_models['XGBoost'] = xgb_model\n",
    "model_predictions['XGBoost'] = y_pred_xgb\n",
    "model_probabilities['XGBoost'] = y_pred_proba_xgb\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model_performance(xgb_model, X_val, y_val, 'XGBoost')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b22e9",
   "metadata": {},
   "source": [
    "## 5. ƒê√°nh gi√° Models v·ªõi evaluate.py\n",
    "\n",
    "S·ª≠ d·ª•ng module evaluate.py ƒë·ªÉ t√≠nh metrics chi ti·∫øt cho t·ª´ng model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e472b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models using evaluate.py\n",
    "all_metrics = []\n",
    "\n",
    "for model_name in trained_models.keys():\n",
    "    y_pred = model_predictions[model_name]\n",
    "    y_pred_proba = model_probabilities[model_name]\n",
    "    \n",
    "    # Get metrics\n",
    "    metrics = get_metrics_dict(y_val, y_pred, y_pred_proba, model_name)\n",
    "    all_metrics.append(metrics)\n",
    "    \n",
    "    # Print detailed metrics\n",
    "    print_metrics(y_val, y_pred, y_pred_proba, model_name)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_metrics)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Metrics Comparison - All Ensemble Models\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb20855e",
   "metadata": {},
   "source": [
    "## 6. Visualize Metrics Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06302c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics comparison\n",
    "from src.evaluate import plot_metrics_comparison\n",
    "\n",
    "fig = plot_metrics_comparison(all_metrics, figsize=(14, 7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66728bbe",
   "metadata": {},
   "source": [
    "## 7. L∆∞u Models ƒë√£ Train\n",
    "\n",
    "L∆∞u t·∫•t c·∫£ models v√†o th∆∞ m·ª•c models/ ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14326930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models\n",
    "models_dir = project_root / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    save_model(model, model_name, save_dir=models_dir)\n",
    "\n",
    "print(f\"\\n‚úÖ All models saved to {models_dir}\")\n",
    "print(f\"‚úÖ Total models saved: {len(trained_models)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c082f58",
   "metadata": {},
   "source": [
    "## 8. T√≥m t·∫Øt\n",
    "\n",
    "T·∫•t c·∫£ models ƒë√£ ƒë∆∞·ª£c train, ƒë√°nh gi√° v√† l∆∞u. K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng trong notebook 03 ƒë·ªÉ so s√°nh v√† ƒë√°nh gi√° cu·ªëi c√πng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Models trained: {len(trained_models)}\")\n",
    "print(f\"   - Random Forest\")\n",
    "print(f\"   - AdaBoost\")\n",
    "print(f\"   - XGBoost\")\n",
    "print(f\"\\n‚úÖ Models evaluated on validation set: {len(y_val):,} samples\")\n",
    "print(f\"‚úÖ Models saved to: {models_dir}\")\n",
    "print(f\"\\nüìä Next steps:\")\n",
    "print(f\"   1. Review metrics comparison above\")\n",
    "print(f\"   2. Run notebook 03 for final evaluation and comparison\")\n",
    "print(f\"   3. Models are ready for use in fraud_detection_app.py\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
