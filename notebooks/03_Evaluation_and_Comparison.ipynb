{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94d4ce1",
   "metadata": {},
   "source": [
    "# Evaluation & Comparison\n",
    "\n",
    "Notebook này thực hiện:\n",
    "1. Kết hợp kết quả từ tất cả models\n",
    "2. Xuất bảng metrics metrics.csv\n",
    "3. Lưu confusion matrix và ROC curves vào thư mục results/\n",
    "4. Review toàn bộ pipeline và dữ liệu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f36ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Get project root directory\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir\n",
    "if (current_dir / 'src').exists():\n",
    "    project_root = current_dir\n",
    "elif (current_dir.parent / 'src').exists():\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "# Add project root to path\n",
    "project_root_str = str(project_root.absolute())\n",
    "if project_root_str not in sys.path:\n",
    "    sys.path.insert(0, project_root_str)\n",
    "\n",
    "# Import evaluation functions\n",
    "from src.evaluate import (\n",
    "    evaluate_model, get_metrics_dict, export_metrics_to_csv,\n",
    "    plot_metrics_comparison, compare_models_roc, print_metrics\n",
    ")\n",
    "\n",
    "print(f\"Project root: {project_root.absolute()}\")\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a375126",
   "metadata": {},
   "source": [
    "## 1. Load Results từ tất cả Models\n",
    "\n",
    "Load predictions và metrics từ các models đã train trong notebook 02.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e31dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for final evaluation\n",
    "# Note: In practice, you would load preprocessed data from notebook 01\n",
    "# For this notebook, we'll reload and preprocess\n",
    "\n",
    "data_path = project_root / 'data' / 'creditcard.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [f'V{i}' for i in range(1, 29)] + ['Amount']\n",
    "X = df[feature_cols]\n",
    "y = df['Class']\n",
    "\n",
    "# Load scaler if available, otherwise scale again\n",
    "scaler_path = project_root / 'models' / 'scaler.pkl'\n",
    "if scaler_path.exists():\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    X_scaled, _ = scale_features(X, feature_cols=feature_cols, scaler=scaler, fit=False)\n",
    "    print(\"✓ Loaded scaler from file\")\n",
    "else:\n",
    "    from src.data_preprocessing import scale_features\n",
    "    X_scaled, scaler = scale_features(X, feature_cols=feature_cols, fit=True)\n",
    "    print(\"✓ Created new scaler\")\n",
    "\n",
    "# Split data (same random_state as notebook 01)\n",
    "from src.data_preprocessing import split_data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    X_scaled, y, \n",
    "    test_size=0.15, \n",
    "    val_size=0.15, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTest set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Test set class distribution: {y_test.value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and get predictions\n",
    "# Note: In practice, models would be loaded from saved pickle files\n",
    "# For demonstration, we'll train small models here\n",
    "# In real scenario, you would load from: models/random_forest.pkl, models/adaboost.pkl, etc.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from src.data_preprocessing import get_class_weights\n",
    "\n",
    "# Get class weights\n",
    "class_weights = get_class_weights(y_train)\n",
    "\n",
    "# Dictionary to store all results\n",
    "all_results = {\n",
    "    'y_true': y_test,  # Use test set for final evaluation\n",
    "    'predictions': {},\n",
    "    'probabilities': {},\n",
    "    'metrics': []\n",
    "}\n",
    "\n",
    "print(\"Training models for evaluation...\")\n",
    "print(\"(In practice, these would be loaded from saved files)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ac748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(class_weight=class_weights, random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "all_results['predictions']['Logistic Regression'] = lr_model.predict(X_test)\n",
    "all_results['probabilities']['Logistic Regression'] = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Model 2: Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    class_weight=class_weights,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "all_results['predictions']['Random Forest'] = rf_model.predict(X_test)\n",
    "all_results['probabilities']['Random Forest'] = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Model 3: AdaBoost\n",
    "print(\"Training AdaBoost...\")\n",
    "ada_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "all_results['predictions']['AdaBoost'] = ada_model.predict(X_test)\n",
    "all_results['probabilities']['AdaBoost'] = ada_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n✓ All models trained and predictions generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2021049",
   "metadata": {},
   "source": [
    "## 2. Kết hợp và So sánh Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all models\n",
    "y_true = all_results['y_true']\n",
    "\n",
    "for model_name in all_results['predictions'].keys():\n",
    "    y_pred = all_results['predictions'][model_name]\n",
    "    y_pred_proba = all_results['probabilities'][model_name]\n",
    "    \n",
    "    metrics = get_metrics_dict(y_true, y_pred, y_pred_proba, model_name)\n",
    "    all_results['metrics'].append(metrics)\n",
    "    print_metrics(y_true, y_pred, y_pred_proba, model_name)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "metrics_df = pd.DataFrame(all_results['metrics'])\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Metrics Comparison - All Models\")\n",
    "print(\"=\"*70)\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ea31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig = plot_metrics_comparison(all_results['metrics'], figsize=(14, 7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8b439",
   "metadata": {},
   "source": [
    "## 3. Xuất metrics.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65645588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export metrics to CSV\n",
    "metrics_csv_path = project_root / 'results' / 'metrics.csv'\n",
    "export_metrics_to_csv(all_results['metrics'], metrics_csv_path)\n",
    "\n",
    "# Display the saved CSV\n",
    "print(\"\\nSaved metrics:\")\n",
    "print(pd.read_csv(metrics_csv_path).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c921ab",
   "metadata": {},
   "source": [
    "## 4. Lưu Confusion Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb93285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for confusion matrices\n",
    "confusion_matrix_dir = project_root / 'results' / 'confusion_matrices'\n",
    "confusion_matrix_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate and save confusion matrices for all models\n",
    "from src.evaluate import plot_confusion_matrix\n",
    "\n",
    "for model_name in all_results['predictions'].keys():\n",
    "    y_pred = all_results['predictions'][model_name]\n",
    "    save_path = confusion_matrix_dir / f'{model_name.replace(\" \", \"_\")}_confusion_matrix.png'\n",
    "    \n",
    "    fig = plot_confusion_matrix(\n",
    "        y_true, y_pred, \n",
    "        model_name=model_name,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"\\n✓ All confusion matrices saved to {confusion_matrix_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cde1e2",
   "metadata": {},
   "source": [
    "## 5. Lưu ROC Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ROC comparison\n",
    "y_true_dict = {model_name: y_true for model_name in all_results['probabilities'].keys()}\n",
    "y_pred_proba_dict = all_results['probabilities']\n",
    "\n",
    "# Plot ROC curves comparison\n",
    "roc_save_path = project_root / 'results' / 'roc_curves_comparison.png'\n",
    "fig = compare_models_roc(y_true_dict, y_pred_proba_dict, roc_save_path, figsize=(12, 8))\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"\\n✓ ROC curves comparison saved to {roc_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b6c8f",
   "metadata": {},
   "source": [
    "## 6. Tổng hợp và So sánh\n",
    "\n",
    "Review toàn bộ pipeline và dữ liệu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dcaaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTotal models evaluated: {len(all_results['metrics'])}\")\n",
    "print(f\"Test set size: {len(y_true):,} samples\")\n",
    "print(f\"Test set fraud cases: {y_true.sum():,} ({y_true.sum()/len(y_true)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Best Model by Metric:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "metrics_df = pd.DataFrame(all_results['metrics'])\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']:\n",
    "    if metric in metrics_df.columns and metrics_df[metric].notna().any():\n",
    "        best_idx = metrics_df[metric].idxmax()\n",
    "        best_model = metrics_df.loc[best_idx, 'model']\n",
    "        best_value = metrics_df.loc[best_idx, metric]\n",
    "        print(f\"{metric.capitalize():12s}: {best_model:25s} ({best_value:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Pipeline Review:\")\n",
    "print(\"=\"*70)\n",
    "print(\"✓ Data preprocessing: Scaling, train/val/test split completed\")\n",
    "print(\"✓ Class imbalance handling: Class weights applied\")\n",
    "print(\"✓ Models trained and evaluated on test set\")\n",
    "print(\"✓ Metrics calculated and exported to CSV\")\n",
    "print(\"✓ Confusion matrices saved\")\n",
    "print(\"✓ ROC curves comparison saved\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c55d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final metrics table\n",
    "print(\"\\nFinal Metrics Table:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Save comparison chart\n",
    "comparison_chart_path = project_root / 'results' / 'metrics_comparison.png'\n",
    "fig = plot_metrics_comparison(all_results['metrics'], comparison_chart_path, figsize=(14, 7))\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"\\n✓ Metrics comparison chart saved to {comparison_chart_path}\")\n",
    "print(\"\\n✅ Evaluation and Comparison completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
