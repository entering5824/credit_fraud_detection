{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bd9819",
   "metadata": {},
   "source": [
    "# EDA và Data Preprocessing cho Credit Card Fraud Detection\n",
    "\n",
    "Notebook này thực hiện:\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Data Preprocessing sử dụng các functions từ `src/data_preprocessing.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Get project root directory\n",
    "# Try to find project root by looking for src directory\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir\n",
    "\n",
    "# Check if we're in notebooks directory or project root\n",
    "if (current_dir / 'src').exists():\n",
    "    # We're in project root\n",
    "    project_root = current_dir\n",
    "elif (current_dir.parent / 'src').exists():\n",
    "    # We're in notebooks directory\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    # Fallback: assume we're in project root\n",
    "    project_root = current_dir\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root_str = str(project_root.absolute())\n",
    "if project_root_str not in sys.path:\n",
    "    sys.path.insert(0, project_root_str)\n",
    "\n",
    "# Import preprocessing functions\n",
    "from src.data_preprocessing import scale_features, split_data, apply_smote, get_class_weights\n",
    "\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "print(f\"Project root: {project_root.absolute()}\")\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b955894",
   "metadata": {},
   "source": [
    "## 1. Load và Kiểm tra Dữ liệu Cơ bản\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = project_root / 'data' / 'creditcard.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Shape of dataset: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dd2e5",
   "metadata": {},
   "source": [
    "## 2. Kiểm tra Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3ff2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"Missing Values Found:\")\n",
    "    print(missing_df)\n",
    "    # Visualize missing values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=missing_df.reset_index(), x='index', y='Missing Percentage')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Missing Percentage (%)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"✓ No missing values found in the dataset!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653b714",
   "metadata": {},
   "source": [
    "## 3. Kiểm tra Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check outliers for Amount feature using IQR method\n",
    "Q1 = df['Amount'].quantile(0.25)\n",
    "Q3 = df['Amount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_iqr = df[(df['Amount'] < lower_bound) | (df['Amount'] > upper_bound)]\n",
    "print(f\"Outliers detected using IQR method: {len(outliers_iqr)} ({len(outliers_iqr)/len(df)*100:.2f}%)\")\n",
    "print(f\"Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}\")\n",
    "\n",
    "# Check outliers using Z-score method\n",
    "from scipy import stats\n",
    "z_scores = np.abs(stats.zscore(df['Amount']))\n",
    "outliers_zscore = df[z_scores > 3]\n",
    "print(f\"\\nOutliers detected using Z-score method (|z| > 3): {len(outliers_zscore)} ({len(outliers_zscore)/len(df)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for Amount\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Box plot\n",
    "axes[0].boxplot(df['Amount'], vert=True)\n",
    "axes[0].set_title('Box Plot of Amount (with outliers)')\n",
    "axes[0].set_ylabel('Amount')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram\n",
    "axes[1].hist(df['Amount'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(lower_bound, color='r', linestyle='--', label=f'Lower bound: {lower_bound:.2f}')\n",
    "axes[1].axvline(upper_bound, color='r', linestyle='--', label=f'Upper bound: {upper_bound:.2f}')\n",
    "axes[1].set_title('Distribution of Amount')\n",
    "axes[1].set_xlabel('Amount')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log transform visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].hist(df['Amount'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Original Amount Distribution')\n",
    "axes[0].set_xlabel('Amount')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Log transformed (add small value to avoid log(0))\n",
    "axes[1].hist(np.log1p(df['Amount']), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Log-transformed Amount Distribution')\n",
    "axes[1].set_xlabel('Log(Amount + 1)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d47c51",
   "metadata": {},
   "source": [
    "## 4. Phân phối các Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Amount by Class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "df[df['Class'] == 0]['Amount'].hist(bins=50, alpha=0.7, label='Normal (0)', ax=axes[0])\n",
    "df[df['Class'] == 1]['Amount'].hist(bins=50, alpha=0.7, label='Fraud (1)', ax=axes[0])\n",
    "axes[0].set_title('Distribution of Amount by Class')\n",
    "axes[0].set_xlabel('Amount')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "df.boxplot(column='Amount', by='Class', ax=axes[1])\n",
    "axes[1].set_title('Amount Distribution by Class')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Amount')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics by class\n",
    "print(\"Amount Statistics by Class:\")\n",
    "print(df.groupby('Class')['Amount'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ccda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some V features to visualize (V1-V28 are PCA transformed)\n",
    "# Let's visualize a few representative features\n",
    "sample_features = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10']\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(sample_features):\n",
    "    df[df['Class'] == 0][feature].hist(bins=50, alpha=0.7, label='Normal', ax=axes[i], color='blue')\n",
    "    df[df['Class'] == 1][feature].hist(bins=50, alpha=0.7, label='Fraud', ax=axes[i], color='red')\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65edaa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions for all V features (density plots)\n",
    "v_features = [f'V{i}' for i in range(1, 29)]\n",
    "\n",
    "# Select a subset for visualization (every 3rd feature to avoid overcrowding)\n",
    "selected_v_features = v_features[::3]  # V1, V4, V7, V10, V13, V16, V19, V22, V25, V28\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(selected_v_features):\n",
    "    df[df['Class'] == 0][feature].plot.density(ax=axes[i], label='Normal', color='blue', alpha=0.7)\n",
    "    df[df['Class'] == 1][feature].plot.density(ax=axes[i], label='Fraud', color='red', alpha=0.7)\n",
    "    axes[i].set_title(f'Density Plot of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d591846",
   "metadata": {},
   "source": [
    "## 5. Kiểm tra Tỉ lệ Fraud/Normal (Class Imbalance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c63142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = df['Class'].value_counts()\n",
    "class_percentages = df['Class'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(f\"Normal (0): {class_counts[0]:,} ({class_percentages[0]:.2f}%)\")\n",
    "print(f\"Fraud (1): {class_counts[1]:,} ({class_percentages[1]:.2f}%)\")\n",
    "print(f\"\\nImbalance Ratio: {class_counts[0]/class_counts[1]:.2f}:1 (Normal:Fraud)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar chart\n",
    "class_counts.plot(kind='bar', ax=axes[0], color=['blue', 'red'], alpha=0.7)\n",
    "axes[0].set_title('Class Distribution (Bar Chart)')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Normal (0)', 'Fraud (1)'], rotation=0)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(class_counts):\n",
    "    axes[0].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "class_counts.plot(kind='pie', ax=axes[1], autopct='%1.2f%%', colors=['blue', 'red'], startangle=90)\n",
    "axes[1].set_title('Class Distribution (Pie Chart)')\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].legend(['Normal (0)', 'Fraud (1)'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n⚠️ Dataset is highly imbalanced!\")\n",
    "print(\"This will require special handling during model training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61430202",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing\n",
    "\n",
    "Sử dụng các functions từ `src/data_preprocessing.py` để thực hiện preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = [f'V{i}' for i in range(1, 29)] + ['Amount']\n",
    "X = df[feature_cols]\n",
    "y = df['Class']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {feature_cols[:5]}... (total {len(feature_cols)} features)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce320b5",
   "metadata": {},
   "source": [
    "### 6.1 Scaling Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb45fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using StandardScaler\n",
    "X_scaled, scaler = scale_features(X, feature_cols=feature_cols, fit=True)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"Scaled features shape: {X_scaled.shape}\")\n",
    "print(f\"\\nScaler statistics:\")\n",
    "print(f\"Mean: {scaler.mean_[:5]}... (showing first 5)\")\n",
    "print(f\"Scale: {scaler.scale_[:5]}... (showing first 5)\")\n",
    "\n",
    "# Verify scaling\n",
    "print(\"\\nVerification - Statistics after scaling:\")\n",
    "print(X_scaled[feature_cols].describe().loc[['mean', 'std']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca01139",
   "metadata": {},
   "source": [
    "### 6.2 Train/Validation/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3967e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 70% train, 15% validation, 15% test\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    X_scaled, y, \n",
    "    test_size=0.15, \n",
    "    val_size=0.15, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data split successfully!\")\n",
    "print(f\"\\nTrain set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in each split\n",
    "print(\"\\nClass distribution in each split:\")\n",
    "print(\"\\nTrain set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"Fraud ratio: {y_train.value_counts()[1]/len(y_train)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "print(y_val.value_counts())\n",
    "print(f\"Fraud ratio: {y_val.value_counts()[1]/len(y_val)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "print(y_test.value_counts())\n",
    "print(f\"Fraud ratio: {y_test.value_counts()[1]/len(y_test)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f5b61",
   "metadata": {},
   "source": [
    "### 6.3 Kế hoạch Xử lý Imbalance\n",
    "\n",
    "Có 2 phương pháp chính để xử lý class imbalance:\n",
    "\n",
    "1. **SMOTE (Synthetic Minority Oversampling Technique)**: Tạo synthetic samples cho minority class\n",
    "2. **Class Weights**: Điều chỉnh trọng số trong quá trình training\n",
    "\n",
    "Cả 2 methods đã được implement trong `src/data_preprocessing.py`. Dưới đây là demo cách sử dụng:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d47b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Calculate class weights\n",
    "class_weights = get_class_weights(y_train)\n",
    "print(\"Class Weights (for use with class_weight parameter in models):\")\n",
    "print(class_weights)\n",
    "print(\"\\nUsage example:\")\n",
    "print(\"model = RandomForestClassifier(class_weight=class_weights)\")\n",
    "print(\"model.fit(X_train, y_train)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Apply SMOTE (only on training data!)\n",
    "# Note: SMOTE should ONLY be applied to training data, not validation/test\n",
    "print(\"Applying SMOTE to training data...\")\n",
    "X_train_smote, y_train_smote = apply_smote(X_train, y_train, random_state=42)\n",
    "\n",
    "print(f\"\\nBefore SMOTE:\")\n",
    "print(f\"  Shape: {X_train.shape}\")\n",
    "print(f\"  Class distribution: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nAfter SMOTE:\")\n",
    "print(f\"  Shape: {X_train_smote.shape}\")\n",
    "unique, counts = np.unique(y_train_smote, return_counts=True)\n",
    "print(f\"  Class distribution: {dict(zip(unique, counts))}\")\n",
    "\n",
    "print(\"\\n⚠️ Note: Use X_train_smote, y_train_smote for training if using SMOTE\")\n",
    "print(\"⚠️ Keep X_val, y_val and X_test, y_test unchanged for validation and testing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f96de1",
   "metadata": {},
   "source": [
    "### 6.4 So sánh Phương pháp Xử lý Imbalance\n",
    "\n",
    "**SMOTE:**\n",
    "- ✅ Tăng số lượng samples cho minority class\n",
    "- ✅ Tạo synthetic samples thay vì duplicate\n",
    "- ❌ Có thể tạo noisy samples nếu minority class quá nhỏ\n",
    "- ❌ Tốn thời gian tính toán\n",
    "- ❌ Chỉ nên áp dụng trên training data\n",
    "\n",
    "**Class Weights:**\n",
    "- ✅ Không cần thay đổi dữ liệu\n",
    "- ✅ Nhanh hơn SMOTE\n",
    "- ✅ Hoạt động tốt với tree-based models\n",
    "- ✅ Giữ nguyên phân phối dữ liệu gốc\n",
    "- ❌ Có thể không hiệu quả bằng SMOTE cho một số models\n",
    "- ❌ Cần model hỗ trợ class_weight parameter\n",
    "\n",
    "**Khuyến nghị:**\n",
    "- Thử cả 2 phương pháp và so sánh kết quả\n",
    "- Với dataset này (imbalance ratio ~577:1), nên thử cả SMOTE và class_weight\n",
    "- Có thể kết hợp cả 2: SMOTE + class_weight cho một số models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba248a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89e5d904",
   "metadata": {},
   "source": [
    "## 7. Lưu Preprocessed Data (Optional)\n",
    "\n",
    "Có thể lưu các preprocessed data và scaler để sử dụng sau này.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler (optional - for future use)\n",
    "import pickle\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "models_dir = project_root / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = models_dir / 'scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"Scaler saved to '{scaler_path}'\")\n",
    "print(\"\\nPreprocessing completed successfully!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use X_train, y_train (or X_train_smote, y_train_smote) for training\")\n",
    "print(\"2. Use X_val, y_val for validation during training\")\n",
    "print(\"3. Use X_test, y_test for final evaluation\")\n",
    "print(\"4. Use class_weights or SMOTE to handle imbalance\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
